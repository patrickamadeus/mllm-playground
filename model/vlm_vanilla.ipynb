{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eeb3a86",
   "metadata": {},
   "source": [
    "# nanoVLM playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b9256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_model, save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce23e5",
   "metadata": {},
   "source": [
    "# VANILLA & PREP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638cacc",
   "metadata": {},
   "source": [
    "## image transforms function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9362ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torchvision.transforms.functional import resize, InterpolationMode\n",
    "from einops import rearrange\n",
    "from typing import Tuple, Union\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class DynamicResize(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Resize so that:\n",
    "      * the longer side ≤ `max_side_len` **and** is divisible by `patch_size`\n",
    "      * the shorter side keeps aspect ratio and is also divisible by `patch_size`\n",
    "    Optionally forbids up-scaling.\n",
    "\n",
    "    Works on PIL Images, (C, H, W) tensors, or (B, C, H, W) tensors.\n",
    "    Returns the same type it receives.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int,\n",
    "        max_side_len: int,\n",
    "        resize_to_max_side_len: bool = False,\n",
    "        interpolation: InterpolationMode = InterpolationMode.BICUBIC,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.p = int(patch_size)\n",
    "        self.m = int(max_side_len)\n",
    "        self.interpolation = interpolation\n",
    "        print(f\"Resize to max side len: {resize_to_max_side_len}\")\n",
    "        self.resize_to_max_side_len = resize_to_max_side_len\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def _get_new_hw(self, h: int, w: int) -> Tuple[int, int]:\n",
    "        \"\"\"Compute target (h, w) divisible by patch_size.\"\"\"\n",
    "        long, short = (w, h) if w >= h else (h, w)\n",
    "\n",
    "        # 1) upscale long side\n",
    "        target_long = self.m if self.resize_to_max_side_len else min(self.m, math.ceil(long / self.p) * self.p)\n",
    "\n",
    "        # 2) scale factor\n",
    "        scale = target_long / long\n",
    "\n",
    "        # 3) compute short side with ceil → never undershoot\n",
    "        target_short = math.ceil(short * scale / self.p) * self.p\n",
    "        target_short = max(target_short, self.p)  # just in case\n",
    "\n",
    "        return (target_short, target_long) if w >= h else (target_long, target_short)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def forward(self, img: Union[Image.Image, torch.Tensor]):\n",
    "        if isinstance(img, Image.Image):\n",
    "            w, h = img.size\n",
    "            new_h, new_w = self._get_new_hw(h, w)\n",
    "            return resize(img, [new_h, new_w], interpolation=self.interpolation)\n",
    "\n",
    "        if not torch.is_tensor(img):\n",
    "            raise TypeError(\n",
    "                \"DynamicResize expects a PIL Image or a torch.Tensor; \"\n",
    "                f\"got {type(img)}\"\n",
    "            )\n",
    "\n",
    "        # tensor path ---------------------------------------------------------\n",
    "        batched = img.ndim == 4\n",
    "        if img.ndim not in (3, 4):\n",
    "            raise ValueError(\n",
    "                \"Tensor input must have shape (C,H,W) or (B,C,H,W); \"\n",
    "                f\"got {img.shape}\"\n",
    "            )\n",
    "\n",
    "        # operate batch-wise\n",
    "        imgs = img if batched else img.unsqueeze(0)\n",
    "        _, _, h, w = imgs.shape\n",
    "        new_h, new_w = self._get_new_hw(h, w)\n",
    "        out = resize(imgs, [new_h, new_w], interpolation=self.interpolation)\n",
    "\n",
    "        return out if batched else out.squeeze(0)\n",
    "\n",
    "\n",
    "class SplitImage(torch.nn.Module):\n",
    "    \"\"\"Split (B, C, H, W) image tensor into square patches.\n",
    "\n",
    "    Returns:\n",
    "        patches: (B·n_h·n_w, C, patch_size, patch_size)\n",
    "        grid:    (n_h, n_w)  - number of patches along H and W\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.p = patch_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        if x.ndim == 3:            # add batch dim if missing\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "        if h % self.p or w % self.p:\n",
    "            raise ValueError(f'Image size {(h,w)} not divisible by patch_size {self.p}')\n",
    "\n",
    "        n_h, n_w = h // self.p, w // self.p\n",
    "        patches = rearrange(x, 'b c (nh ph) (nw pw) -> (b nh nw) c ph pw',\n",
    "                            ph=self.p, pw=self.p)\n",
    "        return patches, (n_h, n_w)\n",
    "\n",
    "\n",
    "class GlobalAndSplitImages(torch.nn.Module):\n",
    "    def __init__(self, patch_size: int):\n",
    "        super().__init__()\n",
    "        self.p = patch_size\n",
    "        self.splitter = SplitImage(patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        if x.ndim == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        patches, grid = self.splitter(x)\n",
    "\n",
    "        if grid == (1, 1):\n",
    "            return patches, grid  # Dont add global patch if there is only one patch\n",
    "\n",
    "        global_patch = resize(x, [self.p, self.p])\n",
    "        return torch.cat([global_patch, patches], dim=0), grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910da9d",
   "metadata": {},
   "source": [
    "## logits inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723224bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above top_p\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "        # Always keep the first token\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        \n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a522a1",
   "metadata": {},
   "source": [
    "## getter functions to stitch all helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ea9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "TOKENIZERS_CACHE = {}\n",
    "\n",
    "def get_tokenizer(name, extra_special_tokens=None, chat_template=None):\n",
    "    if name not in TOKENIZERS_CACHE:\n",
    "        tokenizer_init_kwargs = {\"use_fast\": True}\n",
    "        if extra_special_tokens is not None:\n",
    "            tokenizer_init_kwargs[\"extra_special_tokens\"] = extra_special_tokens\n",
    "        if chat_template is not None:\n",
    "            tokenizer_init_kwargs[\"chat_template\"] = chat_template\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name, **tokenizer_init_kwargs,)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        TOKENIZERS_CACHE[name] = tokenizer\n",
    "    return TOKENIZERS_CACHE[name]\n",
    "\n",
    "def get_image_processor(max_img_size, splitted_image_size, resize_to_max_side_len=False):\n",
    "    return transforms.Compose([\n",
    "        DynamicResize(splitted_image_size, max_img_size, resize_to_max_side_len),\n",
    "        transforms.ToTensor(),\n",
    "        GlobalAndSplitImages(splitted_image_size),\n",
    "    ])\n",
    "\n",
    "def get_image_string(tokenizer, splitted_image_counts, mp_image_token_length):\n",
    "    image_string = \"\"\n",
    "    # splitted_image_counts is a list of tuples (n_h, n_w)\n",
    "    for idx, (n_h, n_w) in enumerate(splitted_image_counts):\n",
    "        if len(splitted_image_counts) > 1:\n",
    "            image_string += f\"<image: {idx}>\"\n",
    "        if hasattr(tokenizer, \"global_image_token\"):\n",
    "            image_string += tokenizer.global_image_token\n",
    "            image_string += tokenizer.image_token * mp_image_token_length\n",
    "            if n_h == 1 and n_w == 1:  # If there is only one patch, treat it as the global image\n",
    "                continue\n",
    "        for i in range(n_h):\n",
    "            for j in range(n_w):\n",
    "                image_string += getattr(tokenizer, f'r{i+1}c{j+1}')\n",
    "                image_string += tokenizer.image_token * mp_image_token_length\n",
    "    return image_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5941f",
   "metadata": {},
   "source": [
    "## model components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b2a19",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad043a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L245\n",
    "class ViTPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = cfg.vit_img_size\n",
    "        self.patch_size = cfg.vit_patch_size\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        self.cls_flag = cfg.vit_cls_flag\n",
    "        self.embd_dim = cfg.vit_hidden_dim\n",
    "\n",
    "        # Conv layer to extract the patches\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.embd_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=\"valid\",\n",
    "        )\n",
    "\n",
    "        if self.cls_flag:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))\n",
    "            self.position_embedding = nn.Parameter(torch.rand(1, self.num_patches + 1, self.embd_dim))\n",
    "        else:\n",
    "            self.position_embedding = nn.Parameter(torch.rand(1, self.num_patches, self.embd_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # extract patches\n",
    "        x = x.flatten(2)  # flatten the patches into a single dimension\n",
    "        x = x.transpose(1, 2)  # transpose to (batch_size, num_patches, hidden_dim)\n",
    "\n",
    "        # Add CLS token (according to original ViT Paper) and position embeddings\n",
    "        if self.cls_flag:\n",
    "            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "        return x\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L381\n",
    "# https://github.com/karpathy/nanoGPT/blob/master/model.py#L29\n",
    "class ViTMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = cfg.vit_n_heads\n",
    "        self.embd_dim = cfg.vit_hidden_dim\n",
    "        assert self.embd_dim % self.n_heads == 0, \"embd_dim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embd_dim // self.n_heads\n",
    "        self.dropout = cfg.vit_dropout\n",
    "\n",
    "        # Combined projections for all heads\n",
    "        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=True)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # Use scaled dot product attention if available\n",
    "        self.sdpa = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.sdpa:\n",
    "            print(\"Warning: scaled dot product attention not available. Using standard attention in ViT.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # Reshape  [B, T, C] -> [B, T, n_heads, head_dim] and transpose -> [B, n_heads, T, head_dim]\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "\n",
    "        if self.sdpa:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, \n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=False # ViT attention is bidirectional\n",
    "            )\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v  # (B, n_heads, T, T) x (B, n_heads, T, head_dim) -> (B, n_heads, T, head_dim)\n",
    "        \n",
    "        # Transpose back from [B, n_heads, T, head_dim] to [B, T, n_heads * head_dim] and combine all heads to [B, T, C]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  \n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L453\n",
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.GELU(approximate='tanh')\n",
    "        self.fc1 = nn.Linear(cfg.vit_hidden_dim, cfg.vit_inter_dim)\n",
    "        self.fc2 = nn.Linear(cfg.vit_inter_dim, cfg.vit_hidden_dim)\n",
    "        self.dropout = nn.Dropout(cfg.vit_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# https://github.com/karpathy/nanoGPT/blob/master/model.py#L94    \n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "        self.attn = ViTMultiHeadAttention(cfg)\n",
    "        self.ln2 = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "        self.mlp = ViTMLP(cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.patch_embedding = ViTPatchEmbeddings(cfg)\n",
    "        self.cls_flag = cfg.vit_cls_flag\n",
    "        self.dropout = nn.Dropout(cfg.vit_dropout)\n",
    "        self.blocks = nn.ModuleList([ViTBlock(cfg) for _ in range(cfg.vit_n_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x) \n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        if self.cls_flag:\n",
    "            x = self.layer_norm(x[:, 0])\n",
    "        else:\n",
    "            x = self.layer_norm(x)\n",
    "            #x = x.mean(dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11dea5",
   "metadata": {},
   "source": [
    "### LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abf47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L69\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(cfg.lm_hidden_dim))\n",
    "        self.eps = cfg.lm_rms_eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute inverse of RMS: square the tensor element-wise, mean is computed across lm_hidden_dim.\n",
    "        irms = torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps) # inverse of RMS\n",
    "        x = x * irms * self.weight\n",
    "\n",
    "        return x\n",
    "\n",
    "# Multiple derivates of Rotary Embeddings by now, this is a basic one with linear scaling to context length\n",
    "# e.g. https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L190\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg.lm_hidden_dim % cfg.lm_n_heads == 0, \"Hidden dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.dim = cfg.lm_hidden_dim // cfg.lm_n_heads # dim of each head\n",
    "        self.base = cfg.lm_re_base\n",
    "        self.max_seq_len = cfg.lm_max_position_embeddings\n",
    "        # Standard RoPE implementation - create frequencies for each dimension\n",
    "        # freq_i = 1 / (base^(2i/dim)) where i is the dimension index\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.original_max_seq_len = cfg.lm_max_position_embeddings\n",
    "        self.attention_scaling = cfg.lm_attn_scaling\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, position_ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, seq_len = position_ids.shape\n",
    "        # Dynamic scaling for longer sequences\n",
    "        # Divide the angle frequency to fit more rotation into the embedding space.\n",
    "        max_seq = position_ids.max() + 1\n",
    "        if max_seq > self.original_max_seq_len:\n",
    "            scale = max_seq / self.original_max_seq_len\n",
    "            inv_freq = self.inv_freq / scale\n",
    "        else:\n",
    "            inv_freq = self.inv_freq\n",
    "            \n",
    "        # Compute theta = position * frequency\n",
    "        # Flatten position_ids for batch processing\n",
    "        flat_position_ids = position_ids.reshape(-1).float()\n",
    "        \n",
    "        # Element-wise outer product: [seq_len] x [dim/2] => [seq_len, dim/2]\n",
    "        freqs = flat_position_ids.unsqueeze(-1) * inv_freq.unsqueeze(0)\n",
    "        \n",
    "        # Reshape to include batch dimension\n",
    "        freqs = freqs.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Now create interleaved pattern\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        \n",
    "        # Compute cos and sin\n",
    "        cos = torch.cos(emb) * self.attention_scaling\n",
    "        sin = torch.sin(emb) * self.attention_scaling\n",
    "        \n",
    "        return cos, sin\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Rotates the input by dividing the hidden dimension to two, then swapping and negating dimensions.\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Apply rotary position embeddings to queries and keys.\n",
    "def apply_rotary_pos_embd(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim:int=1)-> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Applies rotary positional embeddings to query and key tensors in attention mechanisms.\n",
    "\n",
    "    Rotary positional embeddings inject position-dependent rotations into query and key vectors,\n",
    "    enabling transformers to encode positional information effectively without explicit positional encoding.\n",
    "\n",
    "    Args:\n",
    "        q (torch.Tensor): Query tensor with shape [batch_size, num_heads, seq_len, head_dim].\n",
    "        k (torch.Tensor): Key tensor with shape [batch_size, num_heads, seq_len, head_dim].\n",
    "        cos (torch.Tensor): Precomputed cosine positional embeddings with shape [batch_size, seq_len, head_dim].\n",
    "        sin (torch.Tensor): Precomputed sine positional embeddings with shape [batch_size, seq_len, head_dim].\n",
    "        unsqueeze_dim (int, optional): Dimension index to unsqueeze `cos` and `sin` to enable broadcasting.\n",
    "                                      Defaults to 1 (typically the heads dimension).\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: The rotated query and key tensors (`q_embed`, `k_embed`), \n",
    "                                           each with the same shape as the input tensors.\n",
    "\n",
    "    How it works:\n",
    "        - `cos` and `sin` tensors are unsqueezed at `unsqueeze_dim` to broadcast across attention heads.\n",
    "        - Rotary embeddings apply a complex number rotation in the embedding space using:\n",
    "            rotated = (original * cos) + (rotate_half(original) * sin)\n",
    "        - `rotate_half` performs a specific half-dimension rotation on the input tensor.\n",
    "        - This operation encodes relative position information in q and k without adding explicit positional vectors.\n",
    "\n",
    "    Example:\n",
    "        q_embed, k_embed = apply_rotary_pos_embd(q, k, cos, sin)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # We need to make sure cos and sin can be properly broadcast\n",
    "    # to the shape of q and k by adding the heads dimension\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)  # [batch_size, 1, seq_len, head_dim]\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)  # [batch_size, 1, seq_len, head_dim]\n",
    "    \n",
    "    # Apply complex multiplication:\n",
    "    # (q * cos) + (rotate_half(q) * sin)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L214\n",
    "# https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L382\n",
    "class LanguageModelGroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Grouped Query Attention (GQA) as used in some transformer-based language models.\n",
    "\n",
    "    GQA reduces computation by using fewer key-value heads than query heads,\n",
    "    grouping multiple query heads to share the same key-value heads.\n",
    "\n",
    "    Args:\n",
    "        cfg: Configuration object containing:\n",
    "            - lm_n_heads (int): Number of query heads.\n",
    "            - lm_n_kv_heads (int): Number of key-value heads.\n",
    "            - lm_hidden_dim (int): Hidden embedding dimension.\n",
    "            - lm_dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = cfg.lm_n_heads\n",
    "        self.n_kv_heads = cfg.lm_n_kv_heads\n",
    "        self.embd_dim = cfg.lm_hidden_dim\n",
    "        self.dropout = cfg.lm_dropout\n",
    "\n",
    "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        assert self.embd_dim % self.n_heads == 0, \"embd_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = self.embd_dim // self.n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.embd_dim, self.head_dim * self.n_kv_heads, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embd_dim, self.head_dim * self.n_kv_heads, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # Use scaled dot product attention if available\n",
    "        self.sdpa = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.sdpa:\n",
    "            print(\"Warning: scaled dot product attention not available, using standard attention in LM.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, attention_mask=None, block_kv_cache=None) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Forward pass for grouped query attention.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, T_curr, C), where\n",
    "                        B = batch size,\n",
    "                        T_curr = current sequence length,\n",
    "                        C = embedding dimension.\n",
    "            cos (Tensor): Rotary embedding cosines, shape compatible with q and k.\n",
    "            sin (Tensor): Rotary embedding sines, shape compatible with q and k.\n",
    "            attention_mask (Tensor, optional): Attention mask tensor of shape (B, total_kv_length),\n",
    "                                               with 1 for tokens to attend to and 0 for padding.\n",
    "            block_kv_cache (dict, optional): Cache dict with 'key' and 'value' tensors for autoregressive decoding.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, dict]:\n",
    "                - Output tensor after attention and projection, shape (B, T_curr, C).\n",
    "                - Updated block_kv_cache dict for caching key-value states.\n",
    "        \"\"\"\n",
    "        is_prefill = block_kv_cache is None\n",
    "\n",
    "        B, T_curr, C = x.size() # T_curr is the sequence length of the current input x\n",
    "\n",
    "        q_curr = self.q_proj(x).view(B, T_curr, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T_curr, head_dim)\n",
    "        k_curr = self.k_proj(x).view(B, T_curr, self.n_kv_heads, self.head_dim).transpose(1, 2) # (B, n_kv_heads, T_curr, head_dim)\n",
    "        v_curr = self.v_proj(x).view(B, T_curr, self.n_kv_heads, self.head_dim).transpose(1, 2) # (B, n_kv_heads, T_curr, head_dim)\n",
    "\n",
    "        # Apply rotary embeddings to the current q and k\n",
    "        q, k_rotated = apply_rotary_pos_embd(q_curr, k_curr, cos, sin)\n",
    "\n",
    "        # Check if we can use cached keys and values\n",
    "        if not is_prefill and block_kv_cache['key'] is not None:\n",
    "            # Concatenate with cached K, V\n",
    "            # k_rotated and v_curr are for the new token(s)\n",
    "            k = block_kv_cache['key']\n",
    "            v = block_kv_cache['value']\n",
    "            k = torch.cat([k, k_rotated], dim=2)\n",
    "            v = torch.cat([v, v_curr], dim=2)\n",
    "            block_kv_cache['key'] = k\n",
    "            block_kv_cache['value'] = v\n",
    "        else:\n",
    "            # No cache, this is the first pass (prefill)\n",
    "            k = k_rotated\n",
    "            v = v_curr\n",
    "            block_kv_cache = {'key': k, 'value': v}\n",
    "\n",
    "        # Repeat K, V for Grouped Query Attention\n",
    "        k_exp = k.repeat_interleave(self.n_kv_groups, dim=1) # (B, n_heads, T_kv, head_dim)\n",
    "        v_exp = v.repeat_interleave(self.n_kv_groups, dim=1) # (B, n_heads, T_kv, head_dim)\n",
    "        \n",
    "        T_kv = k_exp.size(2) # Total sequence length of keys/values\n",
    "\n",
    "        # Prepare attention mask for SDPA or manual path\n",
    "        # attention_mask is (B, T_kv_total_length), 1 for attend, 0 for pad\n",
    "        additive_attn_mask = None\n",
    "        if attention_mask is not None:\n",
    "            # The current `attention_mask` parameter is assumed to be `[B, total_sequence_length_kv]`\n",
    "            # Let's make it `[B, 1, 1, T_kv]` for SDPA.\n",
    "            mask_for_keys = attention_mask[:, :T_kv] # Ensure mask matches key length [B, T_kv]\n",
    "            additive_attn_mask = (1.0 - mask_for_keys.unsqueeze(1).unsqueeze(2).float()) * torch.finfo(q.dtype).min\n",
    "            # This additive_attn_mask shape is [B, 1, 1, T_kv]\n",
    "\n",
    "        if self.sdpa and x.device.type != 'mps':\n",
    "            # During decode, no additional masking needed as [1, T_kv] is naturally causal\n",
    "            is_causal = (T_curr == T_kv and T_curr > 1)\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k_exp, v_exp,\n",
    "                attn_mask=additive_attn_mask, \n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=is_causal\n",
    "            )\n",
    "        else:\n",
    "            # Manual attention implementation\n",
    "            attn = torch.matmul(q, k_exp.transpose(2, 3)) / math.sqrt(self.head_dim) # (B, n_heads, T_curr, T_kv)\n",
    "            # During decode: no additional masking needed as [1, T_kv] is naturally causal\n",
    "            if T_curr == T_kv and T_curr > 1:\n",
    "                causal_mask_val = torch.tril(torch.ones(T_curr, T_curr, device=x.device, dtype=torch.bool)).view(1, 1, T_curr, T_curr)\n",
    "                attn = attn.masked_fill(~causal_mask_val, float('-inf'))\n",
    "\n",
    "            if additive_attn_mask is not None: # Additive padding mask\n",
    "                # additive_attn_mask is [B,1,1,T_kv], needs to be broadcast to [B, n_heads, T_curr, T_kv]\n",
    "                attn = attn + additive_attn_mask \n",
    "\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v_exp\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T_curr, C)\n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y, block_kv_cache\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L160\n",
    "class LanguageModelMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the feed-forward network (MLP) block used in transformer-based language models.\n",
    "\n",
    "    This MLP uses a gated activation mechanism where two separate linear projections\n",
    "    are applied to the input: one passed through an activation function (gate_proj),\n",
    "    and the other as is (up_proj). Their element-wise product is then projected back\n",
    "    to the embedding dimension (down_proj).\n",
    "\n",
    "    Args:\n",
    "        cfg: Configuration object containing:\n",
    "            - lm_hidden_dim (int): The embedding dimension size.\n",
    "            - lm_inter_dim (int): The intermediate dimension size for the MLP.\n",
    "\n",
    "    Attributes:\n",
    "        activation_fn (Callable): The activation function used (SiLU).\n",
    "        gate_proj (nn.Linear): Linear projection for gating pathway.\n",
    "        up_proj (nn.Linear): Linear projection for upscaling pathway.\n",
    "        down_proj (nn.Linear): Linear projection for downscaling back to embedding dim.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embd_dim = cfg.lm_hidden_dim\n",
    "        self.inter_dim = cfg.lm_inter_dim\n",
    "\n",
    "        self.activation_fn = F.silu\n",
    "        self.gate_proj = nn.Linear(self.embd_dim, self.inter_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(self.embd_dim, self.inter_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(self.inter_dim, self.embd_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the gated MLP block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length, embd_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_length, embd_dim),\n",
    "                    after gated MLP transformation.\n",
    "        \"\"\"\n",
    "        gate = self.activation_fn(self.gate_proj(x))\n",
    "        x = self.up_proj(x)\n",
    "        x = self.down_proj(gate * x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# https://github.com/meta-llama/llama3/blob/main/llama/model.py#L222\n",
    "class LanguageModelBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.mlp = LanguageModelMLP(cfg)\n",
    "        self.attn = LanguageModelGroupedQueryAttention(cfg)\n",
    "        self.norm1 = RMSNorm(cfg) # Input Norm\n",
    "        self.norm2 = RMSNorm(cfg) # Post Attention Norm\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, attention_mask: torch.Tensor=None, block_kv_cache: dict=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim).\n",
    "            cos (Tensor): Cosine positional embeddings for rotary embedding, shape\n",
    "                matching sequence length and head dimension.\n",
    "            sin (Tensor): Sine positional embeddings for rotary embedding, same shape as cos.\n",
    "            attention_mask (Tensor, optional): Attention mask of shape (batch_size, total_kv_length),\n",
    "                with 1 indicating tokens to attend to and 0 for padding tokens.\n",
    "            block_kv_cache (dict, optional): Key-value cache dict for cached keys and values\n",
    "                during decoding. If None, no cache is used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, dict]: Output tensor after the block (same shape as input),\n",
    "                and the updated key-value cache dictionary.\n",
    "        \"\"\"\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        x, block_kv_cache = self.attn(x, cos, sin, attention_mask, block_kv_cache)\n",
    "        x = res + x\n",
    "\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = res + x\n",
    "\n",
    "        return x, block_kv_cache\n",
    "\n",
    "# https://github.com/meta-llama/llama3/blob/main/llama/model.py#L251\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.lm_use_tokens = cfg.lm_use_tokens\n",
    "        self.lm_tie_weights = cfg.lm_tie_weights\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg.lm_vocab_size, cfg.lm_hidden_dim)\n",
    "        self.rotary_embd = RotaryEmbedding(cfg)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            LanguageModelBlock(cfg) for _ in range(cfg.lm_n_blocks)\n",
    "        ])\n",
    "        self.norm = RMSNorm(cfg) # Final Norm\n",
    "        self.head = nn.Linear(cfg.lm_hidden_dim, cfg.lm_vocab_size, bias=False)\n",
    "        if self.lm_tie_weights:\n",
    "            self.head.weight = self.token_embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor=None, kv_cache: list[dict]=None, start_pos: int=0):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the language model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor. If `lm_use_tokens` is True, this should be\n",
    "                token indices with shape (batch_size, sequence_length).\n",
    "                If False, it should be embeddings of shape (batch_size, sequence_length, hidden_dim).\n",
    "            attention_mask (Tensor, optional): Mask tensor for attention to\n",
    "                specify which tokens to attend to, typically of shape\n",
    "                (batch_size, sequence_length). Default is None.\n",
    "            kv_cache (list[dict], optional): List of key-value caches for each transformer\n",
    "                block to enable efficient autoregressive decoding.\n",
    "                If None, no cache is used and new ones are created. Default is None.\n",
    "            start_pos (int, optional): The starting position index for the current input\n",
    "                sequence. Used to compute rotary positional embeddings correctly,\n",
    "                especially for cached sequences during generation. Default is 0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple:\n",
    "                - Tensor: Output logits with shape (batch_size, sequence_length, vocab_size)\n",
    "                if `lm_use_tokens` is True, otherwise the hidden state embeddings\n",
    "                (batch_size, sequence_length, hidden_dim).\n",
    "                - list: Updated list of key-value caches, one for each transformer block,\n",
    "                useful for autoregressive decoding and incremental generation.\n",
    "\n",
    "        Behavior:\n",
    "            - If `lm_use_tokens` is True, the input token indices are first embedded.\n",
    "            - Rotary positional embeddings are generated for the current input positions,\n",
    "            which are passed along to each transformer block.\n",
    "            - For each transformer block, the input is processed along with\n",
    "            rotary embeddings, attention mask, and optional cached key-values.\n",
    "            - After processing all blocks, a final RMS normalization is applied.\n",
    "            - If tokens are used, the normalized hidden states are projected to logits\n",
    "            over the vocabulary.\n",
    "            - The method returns the logits or embeddings along with the updated\n",
    "            cache for efficient decoding.\n",
    "        \"\"\"\n",
    "        if self.lm_use_tokens:\n",
    "            x = self.token_embedding(x)\n",
    "\n",
    "        # T_curr is the length of the current input sequence\n",
    "        B, T_curr, _ = x.size()\n",
    "        \n",
    "        # Create position_ids for the current sequence based on start_pos\n",
    "        current_position_ids = torch.arange(start_pos, start_pos + T_curr, device=x.device).unsqueeze(0).expand(B, -1)\n",
    "        cos, sin = self.rotary_embd(current_position_ids) # Get rotary position embeddings for current tokens\n",
    "\n",
    "        # Initialize new KV cache if none provided\n",
    "        if kv_cache is None:\n",
    "            kv_cache = [None] * len(self.blocks)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, kv_cache[i] = block(x, cos, sin, attention_mask, kv_cache[i])\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Compute logits if we are using tokens, otherwise stay in the embedding space\n",
    "        if self.lm_use_tokens: \n",
    "            x = self.head(x) \n",
    "\n",
    "        return x, kv_cache\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int=20):\n",
    "        \"\"\"\n",
    "        Generate tokens autoregressively from a given input sequence.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor containing token indices or embeddings.\n",
    "                Shape: (batch_size, sequence_length) or (sequence_length,) for a single sequence.\n",
    "            max_new_tokens (int): Number of new tokens to generate after the input sequence.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The generated sequence, including the original inputs and newly generated tokens.\n",
    "                Shape: (batch_size, sequence_length + max_new_tokens)\n",
    "        \"\"\"\n",
    "        # Add batch dimension if needed\n",
    "        if inputs.dim() == 1:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "        generated_outputs = inputs.clone()\n",
    "\n",
    "        prompt_output, kv_cache_list = self.forward(\n",
    "            generated_outputs, \n",
    "            attention_mask=None,\n",
    "            kv_cache=None,\n",
    "            start_pos=0\n",
    "        )\n",
    "        last_output = prompt_output[:, -1, :]\n",
    "\n",
    "        # Decode Phase with KV cache\n",
    "        for i in range(max_new_tokens):\n",
    "            if self.lm_use_tokens:\n",
    "                # Now the model outputs logits\n",
    "                next_output = torch.argmax(last_output, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                # Now the model outputs embeddings\n",
    "                next_output = last_output.unsqueeze(1)\n",
    "\n",
    "            generated_outputs = torch.cat((generated_outputs, next_output), dim=1)\n",
    "            \n",
    "            # The token being processed is `next_token`. Its position is `generated_outputs.size(1) - 1`.\n",
    "            current_token_start_pos = generated_outputs.size(1) - 1\n",
    "\n",
    "            if i == max_new_tokens - 1: \n",
    "                break\n",
    "\n",
    "            decode_step_output, kv_cache_list = self.forward(\n",
    "                next_output, \n",
    "                attention_mask=None,\n",
    "                kv_cache=kv_cache_list,\n",
    "                start_pos=current_token_start_pos\n",
    "            )\n",
    "            last_output = decode_step_output[:, -1, :] \n",
    "    \n",
    "        return generated_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd210a8e",
   "metadata": {},
   "source": [
    "### MLP Proj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99c7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modality Projection from Vision to Language\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModalityProjector(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.input_dim = cfg.vit_hidden_dim * (cfg.mp_pixel_shuffle_factor**2)\n",
    "        self.output_dim = cfg.lm_hidden_dim\n",
    "        self.scale_factor = cfg.mp_pixel_shuffle_factor\n",
    "\n",
    "        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    # https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L1281\n",
    "    def pixel_shuffle(self, x):\n",
    "        bsz, seq, embed_dim = x.size()\n",
    "        seq_root = int(seq**0.5)\n",
    "        assert seq_root**2 == seq # Sequence length must be a perfect square for pixel shuffle\n",
    "        assert seq_root % self.scale_factor == 0 # Sequence root must be divisible by scale factor\n",
    "\n",
    "        height = width = seq_root\n",
    "        x = x.view(bsz, height, width, embed_dim)\n",
    "        h_out = height // self.scale_factor\n",
    "        w_out = width // self.scale_factor\n",
    "        \n",
    "        x = x.reshape(bsz, h_out, self.scale_factor, w_out, self.scale_factor, embed_dim)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        x = x.reshape(bsz, h_out * w_out, embed_dim * self.scale_factor**2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d172540",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43dd6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VLMConfig:\n",
    "    vit_hidden_dim: int = 768\n",
    "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
    "    vit_patch_size: int = 16\n",
    "    vit_img_size: int = 512\n",
    "    vit_n_heads: int = 12\n",
    "    vit_dropout: float = 0.0\n",
    "    vit_n_blocks: int = 12\n",
    "    vit_ln_eps: float = 1e-6\n",
    "    vit_cls_flag: bool = False\n",
    "    vit_model_type: str = 'google/siglip2-base-patch16-512'\n",
    "\n",
    "    lm_hidden_dim: int = 960\n",
    "    lm_inter_dim: int = 2560\n",
    "    lm_rms_eps: float = 1e-5\n",
    "    lm_re_base: int = 100000\n",
    "    lm_max_position_embeddings: int = 8192\n",
    "    lm_base_vocab_size: int = 49152\n",
    "    extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)\n",
    "    lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)\n",
    "    lm_n_heads: int = 15\n",
    "    lm_n_kv_heads: int = 5\n",
    "    lm_dropout: float = 0.0\n",
    "    lm_n_blocks: int = 32\n",
    "    lm_attn_scaling: float = 1.0\n",
    "    lm_max_length: int = 8192\n",
    "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
    "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
    "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M-Instruct' #'HuggingFaceTB/SmolLM2-135M' #\n",
    "    lm_tokenizer: str = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
    "    lm_chat_template: str = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "    mp_pixel_shuffle_factor: int = 4\n",
    "    mp_image_token_length: int = 64\n",
    "\n",
    "    max_img_size: int = 2048\n",
    "    resize_to_max_side_len: bool = True\n",
    "\n",
    "    vlm_extra_tokens: dict[str, str] = field(default_factory=lambda: {\"image_token\": \"<|image|>\", \"global_image_token\": \"<|global_image|>\",\n",
    "      \"r1c1\": \"<row_1_col_1>\", \"r1c2\": \"<row_1_col_2>\", \"r1c3\": \"<row_1_col_3>\", \"r1c4\": \"<row_1_col_4>\", \"r1c5\": \"<row_1_col_5>\", \"r1c6\": \"<row_1_col_6>\", \"r1c7\": \"<row_1_col_7>\", \"r1c8\": \"<row_1_col_8>\",\n",
    "      \"r2c1\": \"<row_2_col_1>\", \"r2c2\": \"<row_2_col_2>\", \"r2c3\": \"<row_2_col_3>\", \"r2c4\": \"<row_2_col_4>\", \"r2c5\": \"<row_2_col_5>\", \"r2c6\": \"<row_2_col_6>\", \"r2c7\": \"<row_2_col_7>\", \"r2c8\": \"<row_2_col_8>\",\n",
    "      \"r3c1\": \"<row_3_col_1>\", \"r3c2\": \"<row_3_col_2>\", \"r3c3\": \"<row_3_col_3>\", \"r3c4\": \"<row_3_col_4>\", \"r3c5\": \"<row_3_col_5>\", \"r3c6\": \"<row_3_col_6>\", \"r3c7\": \"<row_3_col_7>\", \"r3c8\": \"<row_3_col_8>\",\n",
    "      \"r4c1\": \"<row_4_col_1>\", \"r4c2\": \"<row_4_col_2>\", \"r4c3\": \"<row_4_col_3>\", \"r4c4\": \"<row_4_col_4>\", \"r4c5\": \"<row_4_col_5>\", \"r4c6\": \"<row_4_col_6>\", \"r4c7\": \"<row_4_col_7>\", \"r4c8\": \"<row_4_col_8>\",\n",
    "      \"r5c1\": \"<row_5_col_1>\", \"r5c2\": \"<row_5_col_2>\", \"r5c3\": \"<row_5_col_3>\", \"r5c4\": \"<row_5_col_4>\", \"r5c5\": \"<row_5_col_5>\", \"r5c6\": \"<row_5_col_6>\", \"r5c7\": \"<row_5_col_7>\", \"r5c8\": \"<row_5_col_8>\",\n",
    "      \"r6c1\": \"<row_6_col_1>\", \"r6c2\": \"<row_6_col_2>\", \"r6c3\": \"<row_6_col_3>\", \"r6c4\": \"<row_6_col_4>\", \"r6c5\": \"<row_6_col_5>\", \"r6c6\": \"<row_6_col_6>\", \"r6c7\": \"<row_6_col_7>\", \"r6c8\": \"<row_6_col_8>\",\n",
    "      \"r7c1\": \"<row_7_col_1>\", \"r7c2\": \"<row_7_col_2>\", \"r7c3\": \"<row_7_col_3>\", \"r7c4\": \"<row_7_col_4>\", \"r7c5\": \"<row_7_col_5>\", \"r7c6\": \"<row_7_col_6>\", \"r7c7\": \"<row_7_col_7>\", \"r7c8\": \"<row_7_col_8>\",\n",
    "      \"r8c1\": \"<row_8_col_1>\", \"r8c2\": \"<row_8_col_2>\", \"r8c3\": \"<row_8_col_3>\", \"r8c4\": \"<row_8_col_4>\", \"r8c5\": \"<row_8_col_5>\", \"r8c6\": \"<row_8_col_6>\", \"r8c7\": \"<row_8_col_7>\", \"r8c8\": \"<row_8_col_8>\"})\n",
    "    vlm_load_backbone_weights: bool = True\n",
    "    vlm_checkpoint_path: str = 'checkpoints'\n",
    "    hf_repo_name: str = 'nanoVLM'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr_mp: float = 0.00512\n",
    "    lr_vision_backbone: float = 5e-5 #0.0005 #\n",
    "    lr_language_backbone: float = 5e-5 #0\n",
    "    data_cutoff_idx: int = None\n",
    "    val_ratio: float = 0.005\n",
    "    batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    max_grad_norm: float = 1.0\n",
    "    eval_in_epochs: bool = True\n",
    "    eval_interval: int = 500\n",
    "    stats_log_interval: int = 100\n",
    "    max_training_steps: int = 80100\n",
    "    max_images_per_example: int = 8\n",
    "    max_images_per_knapsack: int = 36\n",
    "    max_sample_length: int = 8192\n",
    "    compile: bool = False\n",
    "    resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
    "    train_dataset_path: str = '/fsx/luis_wiedmann/datasets/asterix_rated'\n",
    "    train_dataset_name: tuple[str, ...] = (\"all\", ) #('allava_laion', 'allava_vflan', 'cambrian(filtered)_processed', 'LLaVA_Instruct_150K', 'mmevol', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)') # 'vision_flan(filtered)', 'lvis_instruct4v',\n",
    "    relevance_min_rating: int = 1\n",
    "    image_correspondence_min_rating: int = 1\n",
    "    visual_dependency_min_rating: int = 1\n",
    "    formatting_min_rating: int = 1\n",
    "    wandb_entity: str = \"HuggingFace\" # Indicate the entity to log to in wandb\n",
    "    log_wandb: bool = True\n",
    "    use_lmms_eval: bool = True # Use lmms-eval for evaluation\n",
    "    lmms_eval_tasks: str = 'mmstar,mmmu,ocrbench,textvqa,docvqa,scienceqa,mme,infovqa' # Pass additional task as one string, seperated by commas without spaces (e.g. 'mmstar,mmmu,ocrbench')\n",
    "    lmms_eval_limit: float = None\n",
    "    lmms_eval_batch_size: int = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd7a04",
   "metadata": {},
   "source": [
    "### VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f73182e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_model, save_model\n",
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, cfg: VLMConfig, load_backbone=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if load_backbone:\n",
    "            print(\"Loading from backbone weights\")\n",
    "            self.vision_encoder = ViT.from_pretrained(cfg)\n",
    "            self.decoder = LanguageModel.from_pretrained(cfg)\n",
    "        else:\n",
    "            self.vision_encoder = ViT(cfg)\n",
    "            self.decoder = LanguageModel(cfg)\n",
    "        self.MP = ModalityProjector(cfg)\n",
    "        self.load_backbone = load_backbone\n",
    "        self.tokenizer = get_tokenizer(cfg.lm_tokenizer, cfg.vlm_extra_tokens, cfg.lm_chat_template)\n",
    "\n",
    "    def _replace_img_tokens_with_embd(self, input_ids, token_embd, image_embd):\n",
    "        \"\"\"\n",
    "        Replace every image-token placeholder in `input_ids` with the corresponding slice\n",
    "        from `image_embd`. Supports an arbitrary number of image-token placeholders per sample.\n",
    "        The first example in the batch might have 2 images and the second none.\n",
    "        \"\"\"\n",
    "        # Clone the original embeddings to avoid in-place issues\n",
    "        updated_token_embd = token_embd.clone()\n",
    "\n",
    "        # Build a mask of all image-token positions: shape [B, T_seq]\n",
    "        mask = (input_ids == self.tokenizer.image_token_id)\n",
    "        updated_token_embd[mask] = image_embd.view(-1, image_embd.size(-1)).to(updated_token_embd.dtype) # torch flattens before assigning\n",
    "\n",
    "        return updated_token_embd\n",
    "\n",
    "    def _process_images(self, images, device):\n",
    "        if isinstance(images, list):\n",
    "            if images and isinstance(images[0], list):\n",
    "                images = [img for sublist in images for img in sublist]\n",
    "\n",
    "            if not images:  # Handle cases with no images\n",
    "                return None\n",
    "            else:\n",
    "                return torch.cat(images, dim=0).to(device)\n",
    "        return images # Already a tensor\n",
    "\n",
    "    def forward(self, input_ids, images, attention_mask=None, targets=None):\n",
    "        images_tensor = self._process_images(images, input_ids.device)\n",
    "        token_embd = self.decoder.token_embedding(input_ids) # [B, T_sequence, D_lm]\n",
    "\n",
    "        if images_tensor is not None:\n",
    "            image_embd = self.vision_encoder(images_tensor)\n",
    "            image_embd = self.MP(image_embd)  # [num_images, mp_image_token_length, D_lm]\n",
    "            token_embd = self._replace_img_tokens_with_embd(input_ids, token_embd, image_embd)\n",
    "\n",
    "        logits, _ = self.decoder(token_embd, attention_mask=attention_mask)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = self.decoder.head(logits) # Apply LM head\n",
    "            # Loss is calculated over all tokens, but `targets` (labels) will have -100 for non-answer tokens.\n",
    "            # No need to slice logits based on image embedding size here, as the target mask handles it.\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-100)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, input_ids, images, attention_mask=None, max_new_tokens=5, top_k=50, top_p=0.9, temperature=0.5, greedy=False):\n",
    "        images_tensor = self._process_images(images, input_ids.device)\n",
    "        token_embd = self.decoder.token_embedding(input_ids) # [B, T_prompt_text, D_lm]\n",
    "\n",
    "        if images_tensor is not None:\n",
    "            # 1. Process image if present\n",
    "            image_embd = self.vision_encoder(images_tensor) # [B, T_img_feat, D_model]\n",
    "            image_embd = self.MP(image_embd)      # [B, mp_image_token_length, D_lm]\n",
    "            # 2. Combine image and text embeddings\n",
    "            token_embd = self._replace_img_tokens_with_embd(input_ids, token_embd, image_embd)\n",
    "\n",
    "        current_total_seq_len = token_embd.size(1)\n",
    "        batch_size = input_ids.size(0) # Or token_embd.size(0)\n",
    "        \n",
    "        # --- Multimodal Prefill Phase ---\n",
    "        prefill_output, kv_cache_list = self.decoder(\n",
    "            token_embd,\n",
    "            attention_mask=attention_mask, # Use the provided attention mask\n",
    "            kv_cache=None,\n",
    "            start_pos=0\n",
    "        )\n",
    "        \n",
    "        last_token_output_from_prefill = prefill_output[:, -1, :] \n",
    "        \n",
    "        if not self.decoder.lm_use_tokens:\n",
    "            current_logits = self.decoder.head(last_token_output_from_prefill) \n",
    "        else:\n",
    "            current_logits = last_token_output_from_prefill \n",
    "\n",
    "        # Store newly generated token IDs\n",
    "        newly_generated_ids_list = []\n",
    "\n",
    "        # --- Decode Phase by sampling tokens autoregressively using the kv-cache ---\n",
    "        for _ in range(max_new_tokens):\n",
    "            if greedy:\n",
    "                next_token_id = torch.argmax(current_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                filtered_logits = top_k_top_p_filtering(current_logits, top_k=top_k, top_p=top_p)\n",
    "                probs = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            newly_generated_ids_list.append(next_token_id)\n",
    "            \n",
    "            # Embed the newly generated token\n",
    "            next_token_embed = self.decoder.token_embedding(next_token_id) # [B, 1, D_lm]\n",
    "            \n",
    "            # The start_pos for the new token is the current total sequence length *before* adding this new token\n",
    "            current_token_start_pos = current_total_seq_len\n",
    "            current_total_seq_len += 1\n",
    "\n",
    "            # update attention mask\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = torch.cat((attention_mask, torch.ones((batch_size, 1), device=attention_mask.device, dtype=attention_mask.dtype)), dim=1)\n",
    "\n",
    "            # With KV cache: only process the new token\n",
    "            decode_step_output, kv_cache_list = self.decoder(\n",
    "                next_token_embed,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=kv_cache_list,\n",
    "                start_pos=current_token_start_pos\n",
    "            )\n",
    "      \n",
    "            last_token_output = decode_step_output[:, -1, :] \n",
    "            \n",
    "            # Apply head to get logits (if model is in embedding mode)\n",
    "            if not self.decoder.lm_use_tokens:\n",
    "                current_logits = self.decoder.head(last_token_output)\n",
    "            else:\n",
    "                current_logits = last_token_output\n",
    "        \n",
    "        if not newly_generated_ids_list: # Handle case where max_new_tokens might be 0\n",
    "            return torch.empty((batch_size,0), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        generated_ids = torch.cat(newly_generated_ids_list, dim=1)\n",
    "\n",
    "        # Post-process to handle EOS token.\n",
    "        if self.tokenizer.eos_token_id is not None and generated_ids.numel() > 0: # Ensure generated_ids is not empty\n",
    "            seq_len = generated_ids.size(1)\n",
    "            device = generated_ids.device\n",
    "\n",
    "            eos_mask = (generated_ids == self.tokenizer.eos_token_id) # Create a boolean mask for EOS tokens\n",
    "\n",
    "            col_indices_for_min = torch.arange(seq_len, device=device) # Create column indices [0, 1, ..., seq_len-1]\n",
    "            \n",
    "            # In eos_mask, mark positions with actual col_idx, others with a large number\n",
    "            masked_col_indices = torch.where(eos_mask, col_indices_for_min.unsqueeze(0).expand_as(generated_ids), seq_len + 1) \n",
    "\n",
    "            first_eos_indices_values = torch.min(masked_col_indices, dim=1).values\n",
    "            \n",
    "            # Clamp values to seq_len (if no EOS found, min will be seq_len + 1, clamp brings it to seq_len0. This means if no EOS, or EOS is the last token, no replacement will happen for that sample.\n",
    "            actual_first_eos_indices = torch.clamp(first_eos_indices_values, max=seq_len)\n",
    "\n",
    "            # Create column indices for comparison, shape [batch_size, seq_len]\n",
    "            col_indices_for_comparison = torch.arange(seq_len, device=device).unsqueeze(0).expand_as(generated_ids)\n",
    "            \n",
    "            # Tokens are replaced if their column index is greater than the index of the first EOS token\n",
    "            replace_mask = col_indices_for_comparison > actual_first_eos_indices.unsqueeze(1)\n",
    "            \n",
    "            generated_ids[replace_mask] = self.tokenizer.eos_token_id\n",
    "        \n",
    "        return generated_ids\n",
    "\n",
    "    def save_pretrained(self, save_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model and configuration to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (str): The directory to save the model and config.\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Save config\n",
    "        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n",
    "            f.write(json.dumps(asdict(self.cfg), indent=4))\n",
    "\n",
    "        # Save weights as safetensors\n",
    "        save_model(self, os.path.join(save_directory, \"model.safetensors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26cacd",
   "metadata": {},
   "source": [
    "## Dry Run Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a0214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a07558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionLanguageModel(\n",
       "  (vision_encoder): ViT(\n",
       "    (patch_embedding): ViTPatchEmbeddings(\n",
       "      (conv): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x ViTBlock(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): ViTMultiHeadAttention(\n",
       "          (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): ViTMLP(\n",
       "          (activation_fn): GELU(approximate='tanh')\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): LanguageModel(\n",
       "    (token_embedding): Embedding(49218, 960)\n",
       "    (rotary_embd): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x LanguageModelBlock(\n",
       "        (mlp): LanguageModelMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "        )\n",
       "        (attn): LanguageModelGroupedQueryAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm1): RMSNorm()\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (head): Linear(in_features=960, out_features=49218, bias=False)\n",
       "  )\n",
       "  (MP): ModalityProjector(\n",
       "    (proj): Linear(in_features=12288, out_features=960, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = VLMConfig()\n",
    "model = VisionLanguageModel(cfg, load_backbone=False)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0369238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize to max side len: True\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Get tokenizer and image processor from model config\n",
    "tokenizer = get_tokenizer(model.cfg.lm_tokenizer, model.cfg.vlm_extra_tokens, model.cfg.lm_chat_template)\n",
    "resize_to_max_side_len = False\n",
    "if hasattr(model.cfg, \"resize_to_max_side_len\"):\n",
    "    resize_to_max_side_len = model.cfg.resize_to_max_side_len\n",
    "image_processor = get_image_processor(model.cfg.max_img_size, model.cfg.vit_img_size, resize_to_max_side_len)\n",
    "\n",
    "# Load and process the image\n",
    "img = Image.open(\"../files/image.png\").convert(\"RGB\")\n",
    "processed_image, splitted_image_ratio = image_processor(img)\n",
    "if not hasattr(tokenizer, \"global_image_token\") and splitted_image_ratio[0]*splitted_image_ratio[1] == len(processed_image) - 1:\n",
    "    # If the tokenizer doesn't have a global image token, but the processor generated it, remove it\n",
    "    print(\"Removing global image token\")\n",
    "    processed_image = processed_image[1:]\n",
    "\n",
    "# Create the image string and prompt\n",
    "image_string = get_image_string(tokenizer, [splitted_image_ratio], model.cfg.mp_image_token_length)\n",
    "prompt = \"Describe the following image:\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": image_string + prompt}]\n",
    "encoded_prompt = tokenizer.apply_chat_template([messages], tokenize=True, add_generation_prompt=True)\n",
    "tokens = torch.tensor(encoded_prompt).to(DEVICE)\n",
    "img_t = processed_image.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(tokens, img_t, max_new_tokens=20)\n",
    "\n",
    "output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb06f4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=150x150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\" de text='fm Oncology capturing capturing blo='=' banks de accompaniment JasonHEAD lender marketed inhibitoryTab introspection\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(img.resize((150,150)))\n",
    "display(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80969903",
   "metadata": {},
   "source": [
    "# TWIN TOWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0b301c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_model, save_model\n",
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, cfg: VLMConfig, load_backbone=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # if load_backbone:\n",
    "        #     print(\"Loading from backbone weights\")\n",
    "        #     self.vision_encoder = ViT.from_pretrained(cfg)\n",
    "        #     self.decoder = LanguageModel.from_pretrained(cfg)\n",
    "        # else:\n",
    "        self.vision_encoder = ViT(cfg)\n",
    "        self.decoder = LanguageModel(cfg)\n",
    "        self.MP = ModalityProjector(cfg)\n",
    "        self.load_backbone = load_backbone\n",
    "        self.tokenizer = get_tokenizer(cfg.lm_tokenizer, cfg.vlm_extra_tokens, cfg.lm_chat_template)\n",
    "\n",
    "    def _replace_img_tokens_with_embd(self, input_ids, token_embd, image_embd):\n",
    "        \"\"\"\n",
    "        Replace every image-token placeholder in `input_ids` with the corresponding slice\n",
    "        from `image_embd`. Supports an arbitrary number of image-token placeholders per sample.\n",
    "        The first example in the batch might have 2 images and the second none.\n",
    "        \"\"\"\n",
    "        # Clone the original embeddings to avoid in-place issues\n",
    "        updated_token_embd = token_embd.clone()\n",
    "\n",
    "        # Build a mask of all image-token positions: shape [B, T_seq]\n",
    "        mask = (input_ids == self.tokenizer.image_token_id)\n",
    "        updated_token_embd[mask] = image_embd.view(-1, image_embd.size(-1)).to(updated_token_embd.dtype) # torch flattens before assigning\n",
    "\n",
    "        return updated_token_embd\n",
    "\n",
    "    def _process_images(self, images, device):\n",
    "        if isinstance(images, list):\n",
    "            if images and isinstance(images[0], list):\n",
    "                images = [img for sublist in images for img in sublist]\n",
    "\n",
    "            if not images:  # Handle cases with no images\n",
    "                return None\n",
    "            else:\n",
    "                return torch.cat(images, dim=0).to(device)\n",
    "        return images # Already a tensor\n",
    "\n",
    "    def forward(self, input_ids, images, attention_mask=None, targets=None):\n",
    "        images_tensor = self._process_images(images, input_ids.device)\n",
    "        token_embd = self.decoder.token_embedding(input_ids) # [B, T_sequence, D_lm]\n",
    "\n",
    "        if images_tensor is not None:\n",
    "            image_embd = self.vision_encoder(images_tensor)\n",
    "            image_embd = self.MP(image_embd)  # [num_images, mp_image_token_length, D_lm]\n",
    "            token_embd = self._replace_img_tokens_with_embd(input_ids, token_embd, image_embd)\n",
    "\n",
    "        logits, _ = self.decoder(token_embd, attention_mask=attention_mask)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = self.decoder.head(logits) # Apply LM head\n",
    "            # Loss is calculated over all tokens, but `targets` (labels) will have -100 for non-answer tokens.\n",
    "            # No need to slice logits based on image embedding size here, as the target mask handles it.\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-100)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def extract_image_features(self, input_ids, images, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Extract image features from the VLM tower for use in the twin tower architecture.\n",
    "        Returns the processed image embeddings that can be used by the second tower.\n",
    "        \n",
    "        Shape flow:\n",
    "        - images_tensor: [num_images, C, H, W] -> vision encoder\n",
    "        - image_embd: [num_images, patch_tokens, D_vision] -> modality projector\n",
    "        - final output: [num_images, mp_image_token_length, D_lm]\n",
    "        \"\"\"\n",
    "        images_tensor = self._process_images(images, input_ids.device)\n",
    "        \n",
    "        if images_tensor is None:\n",
    "            return None\n",
    "            \n",
    "        # Process images through vision encoder and modality projector\n",
    "        image_embd = self.vision_encoder(images_tensor)  # [num_images, patch_tokens, D_vision]\n",
    "        image_embd = self.MP(image_embd)  # [num_images, mp_image_token_length, D_lm]\n",
    "        \n",
    "        return image_embd\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, input_ids, images, attention_mask=None, max_new_tokens=5, top_k=50, top_p=0.9, temperature=0.5, greedy=False):\n",
    "        images_tensor = self._process_images(images, input_ids.device)\n",
    "        token_embd = self.decoder.token_embedding(input_ids) # [B, T_prompt_text, D_lm]\n",
    "\n",
    "        if images_tensor is not None:\n",
    "            # 1. Process image if present\n",
    "            image_embd = self.vision_encoder(images_tensor) # [B, T_img_feat, D_model]\n",
    "            image_embd = self.MP(image_embd)      # [B, mp_image_token_length, D_lm]\n",
    "            # 2. Combine image and text embeddings\n",
    "            token_embd = self._replace_img_tokens_with_embd(input_ids, token_embd, image_embd)\n",
    "\n",
    "        current_total_seq_len = token_embd.size(1)\n",
    "        batch_size = input_ids.size(0) # Or token_embd.size(0)\n",
    "        \n",
    "        # --- Multimodal Prefill Phase ---\n",
    "        prefill_output, kv_cache_list = self.decoder(\n",
    "            token_embd,\n",
    "            attention_mask=attention_mask, # Use the provided attention mask\n",
    "            kv_cache=None,\n",
    "            start_pos=0\n",
    "        )\n",
    "        \n",
    "        last_token_output_from_prefill = prefill_output[:, -1, :] \n",
    "        \n",
    "        if not self.decoder.lm_use_tokens:\n",
    "            current_logits = self.decoder.head(last_token_output_from_prefill) \n",
    "        else:\n",
    "            current_logits = last_token_output_from_prefill \n",
    "\n",
    "        # Store newly generated token IDs\n",
    "        newly_generated_ids_list = []\n",
    "\n",
    "        # --- Decode Phase by sampling tokens autoregressively using the kv-cache ---\n",
    "        for _ in range(max_new_tokens):\n",
    "            if greedy:\n",
    "                next_token_id = torch.argmax(current_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                filtered_logits = top_k_top_p_filtering(current_logits, top_k=top_k, top_p=top_p)\n",
    "                probs = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            newly_generated_ids_list.append(next_token_id)\n",
    "            \n",
    "            # Embed the newly generated token\n",
    "            next_token_embed = self.decoder.token_embedding(next_token_id) # [B, 1, D_lm]\n",
    "            \n",
    "            # The start_pos for the new token is the current total sequence length *before* adding this new token\n",
    "            current_token_start_pos = current_total_seq_len\n",
    "            current_total_seq_len += 1\n",
    "\n",
    "            # update attention mask\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = torch.cat((attention_mask, torch.ones((batch_size, 1), device=attention_mask.device, dtype=attention_mask.dtype)), dim=1)\n",
    "\n",
    "            # With KV cache: only process the new token\n",
    "            decode_step_output, kv_cache_list = self.decoder(\n",
    "                next_token_embed,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=kv_cache_list,\n",
    "                start_pos=current_token_start_pos\n",
    "            )\n",
    "      \n",
    "            last_token_output = decode_step_output[:, -1, :] \n",
    "            \n",
    "            # Apply head to get logits (if model is in embedding mode)\n",
    "            if not self.decoder.lm_use_tokens:\n",
    "                current_logits = self.decoder.head(last_token_output)\n",
    "            else:\n",
    "                current_logits = last_token_output\n",
    "        \n",
    "        if not newly_generated_ids_list: # Handle case where max_new_tokens might be 0\n",
    "            return torch.empty((batch_size,0), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        generated_ids = torch.cat(newly_generated_ids_list, dim=1)\n",
    "\n",
    "        # Post-process to handle EOS token.\n",
    "        if self.tokenizer.eos_token_id is not None and generated_ids.numel() > 0: # Ensure generated_ids is not empty\n",
    "            seq_len = generated_ids.size(1)\n",
    "            device = generated_ids.device\n",
    "\n",
    "            eos_mask = (generated_ids == self.tokenizer.eos_token_id) # Create a boolean mask for EOS tokens\n",
    "\n",
    "            col_indices_for_min = torch.arange(seq_len, device=device) # Create column indices [0, 1, ..., seq_len-1]\n",
    "            \n",
    "            # In eos_mask, mark positions with actual col_idx, others with a large number\n",
    "            masked_col_indices = torch.where(eos_mask, col_indices_for_min.unsqueeze(0).expand_as(generated_ids), seq_len + 1) \n",
    "\n",
    "            first_eos_indices_values = torch.min(masked_col_indices, dim=1).values\n",
    "            \n",
    "            # Clamp values to seq_len (if no EOS found, min will be seq_len + 1, clamp brings it to seq_len0. This means if no EOS, or EOS is the last token, no replacement will happen for that sample.\n",
    "            actual_first_eos_indices = torch.clamp(first_eos_indices_values, max=seq_len)\n",
    "\n",
    "            # Create column indices for comparison, shape [batch_size, seq_len]\n",
    "            col_indices_for_comparison = torch.arange(seq_len, device=device).unsqueeze(0).expand_as(generated_ids)\n",
    "            \n",
    "            # Tokens are replaced if their column index is greater than the index of the first EOS token\n",
    "            replace_mask = col_indices_for_comparison > actual_first_eos_indices.unsqueeze(1)\n",
    "            \n",
    "            generated_ids[replace_mask] = self.tokenizer.eos_token_id\n",
    "        \n",
    "        return generated_ids\n",
    "\n",
    "    def save_pretrained(self, save_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model and configuration to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (str): The directory to save the model and config.\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Save config\n",
    "        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n",
    "            f.write(json.dumps(asdict(self.cfg), indent=4))\n",
    "\n",
    "        # Save weights as safetensors\n",
    "        save_model(self, os.path.join(save_directory, \"model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43884b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinTowerVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Twin Tower Vision Language Model Architecture:\n",
    "    - Tower 1 (V1): VLM that processes text + image embeddings\n",
    "    - Tower 2 (V2): Pure LLM that processes text-only sequence with extracted image tokens\n",
    "    \n",
    "    Architecture Flow:\n",
    "    Input: [B, T_seq] text tokens + [num_images, C, H, W] images\n",
    "    Tower 1: Full multimodal processing -> [B, T_seq, D_lm] -> [B, T_seq, vocab_size]\n",
    "    Tower 2: Text-only + extracted image features -> [B, T_new, D_lm] -> [B, T_new, vocab_size]\n",
    "    \"\"\"\n",
    "    def __init__(self, vlm_cfg: VLMConfig, llm_cfg: VLMConfig, load_backbone=True):\n",
    "        super().__init__()\n",
    "        self.vlm_cfg = vlm_cfg\n",
    "        self.llm_cfg = llm_cfg\n",
    "        \n",
    "        # Tower 1: VLM (processes text + images)\n",
    "        self.vlm_tower = VisionLanguageModel(vlm_cfg, load_backbone=load_backbone)\n",
    "        \n",
    "        # Tower 2: Pure LLM (processes text-only with image tokens)\n",
    "        # if load_backbone:\n",
    "        #     print(\"Loading LLM tower from backbone weights\")\n",
    "        #     self.llm_tower = LanguageModel.from_pretrained(llm_cfg)\n",
    "        # else:\n",
    "        self.llm_tower = LanguageModel(llm_cfg)\n",
    "        \n",
    "        self.tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens, vlm_cfg.lm_chat_template)\n",
    "        \n",
    "        # Special token for connecting towers\n",
    "        self.image_feature_token_id = self.tokenizer.convert_tokens_to_ids(\"<|image_features|>\")\n",
    "        if self.image_feature_token_id is None:\n",
    "            # If token doesn't exist, add it\n",
    "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|image_features|>\"]})\n",
    "            self.image_feature_token_id = self.tokenizer.convert_tokens_to_ids(\"<|image_features|>\")\n",
    "\n",
    "    def _extract_text_only_sequence(self, input_ids):\n",
    "        \"\"\"\n",
    "        Extract text-only sequence by removing image token placeholders.\n",
    "        Returns the clean text sequence for Tower 2.\n",
    "        \n",
    "        Shape flow:\n",
    "        - input_ids: [B, T_seq] with image tokens mixed in\n",
    "        - non_image_mask: [B, T_seq] boolean mask where True = text token\n",
    "        - output: [B, T_text_only] padded text-only sequence\n",
    "        \"\"\"\n",
    "        # Create mask for non-image tokens: [B, T_seq] -> boolean\n",
    "        non_image_mask = (input_ids != self.tokenizer.image_token_id)\n",
    "        \n",
    "        # Extract text-only tokens for each batch\n",
    "        text_only_ids = []\n",
    "        for batch_idx in range(input_ids.size(0)):\n",
    "            # Extract non-image tokens: [T_seq] -> [T_text_only]\n",
    "            batch_text_tokens = input_ids[batch_idx][non_image_mask[batch_idx]]\n",
    "            text_only_ids.append(batch_text_tokens)\n",
    "        \n",
    "        # Pad sequences to same length: list of [T_text_only] -> [B, max(T_text_only)]\n",
    "        max_len = max(seq.size(0) for seq in text_only_ids)\n",
    "        padded_text_ids = torch.full((len(text_only_ids), max_len), \n",
    "                                   self.tokenizer.pad_token_id, \n",
    "                                   dtype=input_ids.dtype, \n",
    "                                   device=input_ids.device)\n",
    "        \n",
    "        for i, seq in enumerate(text_only_ids):\n",
    "            padded_text_ids[i, :seq.size(0)] = seq\n",
    "            \n",
    "        return padded_text_ids  # [B, T_text_only]\n",
    "\n",
    "    def _insert_image_features_into_sequence(self, text_ids, image_features):\n",
    "        \"\"\"\n",
    "        Insert image features into the text sequence at appropriate positions.\n",
    "        This creates the input for Tower 2 (LLM).\n",
    "        \n",
    "        Shape flow:\n",
    "        - text_ids: [B, T_text] text-only token IDs\n",
    "        - text_embeddings: [B, T_text, D_lm] after embedding lookup\n",
    "        - image_features: [num_images, mp_image_token_length, D_lm] from Tower 1\n",
    "        - image_feature_token_emb: [B, 1, D_lm] special connector token\n",
    "        - combined_embeddings: [B, 1 + mp_image_token_length + T_text, D_lm] final sequence\n",
    "        \"\"\"\n",
    "        batch_size = text_ids.size(0)\n",
    "        \n",
    "        # Convert text tokens to embeddings: [B, T_text] -> [B, T_text, D_lm]\n",
    "        text_embeddings = self.llm_tower.token_embedding(text_ids)\n",
    "        \n",
    "        if image_features is None:\n",
    "            return text_embeddings  # [B, T_text, D_lm]\n",
    "        \n",
    "        # Create image feature token embeddings: [B, 1] -> [B, 1, D_lm]\n",
    "        image_feature_token_ids = torch.full((batch_size, 1), \n",
    "                                           self.image_feature_token_id,\n",
    "                                           dtype=text_ids.dtype,\n",
    "                                           device=text_ids.device)\n",
    "        image_feature_token_emb = self.llm_tower.token_embedding(image_feature_token_ids)\n",
    "        \n",
    "        # Expand image features to batch dimension if needed\n",
    "        if len(image_features.shape) == 2:\n",
    "            # Single image case: [mp_image_token_length, D_lm] -> [B, mp_image_token_length, D_lm]\n",
    "            image_features = image_features.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate: [B, 1, D_lm] + [B, mp_image_token_length, D_lm] + [B, T_text, D_lm]\n",
    "        # Result: [B, 1 + mp_image_token_length + T_text, D_lm]\n",
    "        combined_embeddings = torch.cat([\n",
    "            image_feature_token_emb,  # [B, 1, D_lm]\n",
    "            image_features,           # [B, mp_image_token_length, D_lm]\n",
    "            text_embeddings          # [B, T_text, D_lm]\n",
    "        ], dim=1)\n",
    "        \n",
    "        return combined_embeddings\n",
    "\n",
    "    def forward(self, input_ids, images, attention_mask=None, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through twin tower architecture:\n",
    "        1. Process through VLM tower to extract image features\n",
    "        2. Create text-only sequence for LLM tower\n",
    "        3. Combine image features with text sequence in LLM tower\n",
    "        4. Return outputs from both towers\n",
    "        \n",
    "        Shape flow:\n",
    "        - input_ids: [B, T_seq] mixed text + image tokens\n",
    "        - images: [num_images, C, H, W] or list of tensors\n",
    "        - vlm_logits: [B, T_seq, vocab_size] from Tower 1\n",
    "        - llm_logits: [B, T_combined, vocab_size] from Tower 2\n",
    "        \"\"\"\n",
    "        # Tower 1: VLM processes full multimodal input\n",
    "        # input_ids [B, T_seq] + images -> vlm_logits [B, T_seq, vocab_size]\n",
    "        vlm_logits, vlm_loss = self.vlm_tower(input_ids, images, attention_mask, targets)\n",
    "        \n",
    "        # Extract image features from VLM tower: images -> [num_images, mp_image_token_length, D_lm]\n",
    "        image_features = self.vlm_tower.extract_image_features(input_ids, images, attention_mask)\n",
    "        \n",
    "        # Tower 2: LLM processes text-only sequence with image features\n",
    "        # Extract text-only: [B, T_seq] -> [B, T_text_only]\n",
    "        text_only_ids = self._extract_text_only_sequence(input_ids)\n",
    "        \n",
    "        # Create combined embeddings: [B, T_text_only] + image_features -> [B, T_combined, D_lm]\n",
    "        combined_embeddings = self._insert_image_features_into_sequence(text_only_ids, image_features)\n",
    "        \n",
    "        # Create attention mask for combined sequence\n",
    "        if attention_mask is not None:\n",
    "            # Text-only mask: [B, T_text_only]\n",
    "            text_only_mask = torch.ones(text_only_ids.shape, dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "            if image_features is not None:\n",
    "                # Image features mask: [B, 1 + mp_image_token_length]\n",
    "                img_feat_mask = torch.ones((text_only_ids.size(0), 1 + image_features.size(1)), \n",
    "                                         dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "                # Combined mask: [B, 1 + mp_image_token_length + T_text_only]\n",
    "                combined_attention_mask = torch.cat([img_feat_mask, text_only_mask], dim=1)\n",
    "            else:\n",
    "                combined_attention_mask = text_only_mask\n",
    "        else:\n",
    "            combined_attention_mask = None\n",
    "        \n",
    "        # Process through LLM tower: [B, T_combined, D_lm] -> [B, T_combined, D_lm] -> [B, T_combined, vocab_size]\n",
    "        llm_output, _ = self.llm_tower(combined_embeddings, attention_mask=combined_attention_mask)\n",
    "        llm_logits = self.llm_tower.head(llm_output)\n",
    "        \n",
    "        # Calculate loss for LLM tower if targets provided\n",
    "        llm_loss = None\n",
    "        if targets is not None:\n",
    "            # Adjust targets for the new sequence structure\n",
    "            # Extract text-only targets: [B, T_seq] -> [B, T_text_only]\n",
    "            text_only_targets = self._extract_text_only_sequence(targets)\n",
    "            if image_features is not None:\n",
    "                # Add -100 (ignore) tokens for image feature positions: [B, 1 + mp_image_token_length]\n",
    "                img_ignore_targets = torch.full((text_only_targets.size(0), 1 + image_features.size(1)), \n",
    "                                              -100, dtype=targets.dtype, device=targets.device)\n",
    "                # Combined targets: [B, 1 + mp_image_token_length + T_text_only]\n",
    "                combined_targets = torch.cat([img_ignore_targets, text_only_targets], dim=1)\n",
    "            else:\n",
    "                combined_targets = text_only_targets\n",
    "            \n",
    "            # Calculate loss: [B, T_combined, vocab_size] vs [B, T_combined]\n",
    "            llm_loss = F.cross_entropy(llm_logits.reshape(-1, llm_logits.size(-1)), \n",
    "                                     combined_targets.reshape(-1), ignore_index=-100)\n",
    "        \n",
    "        return {\n",
    "            'vlm_logits': vlm_logits,      # [B, T_seq, vocab_size]\n",
    "            'vlm_loss': vlm_loss,          # scalar\n",
    "            'llm_logits': llm_logits,      # [B, T_combined, vocab_size]\n",
    "            'llm_loss': llm_loss,          # scalar\n",
    "            'image_features': image_features  # [num_images, mp_image_token_length, D_lm]\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, input_ids, images, attention_mask=None, max_new_tokens=5, \n",
    "                top_k=50, top_p=0.9, temperature=0.5, greedy=False, use_tower='both'):\n",
    "        \"\"\"\n",
    "        Generate text using twin tower architecture.\n",
    "        \n",
    "        Args:\n",
    "            use_tower: 'vlm', 'llm', or 'both' - which tower(s) to use for generation\n",
    "            \n",
    "        Shape flow depends on use_tower:\n",
    "        - 'vlm': input_ids [B, T_seq] -> generated_ids [B, max_new_tokens]\n",
    "        - 'llm': input_ids [B, T_seq] -> text_only [B, T_text] -> combined [B, T_combined] -> generated_ids [B, max_new_tokens]\n",
    "        - 'both': returns dict with both outputs\n",
    "        \"\"\"\n",
    "        if use_tower == 'vlm':\n",
    "            return self.vlm_tower.generate(input_ids, images, attention_mask, \n",
    "                                         max_new_tokens, top_k, top_p, temperature, greedy)\n",
    "        \n",
    "        elif use_tower == 'llm':\n",
    "            # Extract image features from VLM tower: images -> [num_images, mp_image_token_length, D_lm]\n",
    "            image_features = self.vlm_tower.extract_image_features(input_ids, images, attention_mask)\n",
    "            \n",
    "            # Create text-only sequence: [B, T_seq] -> [B, T_text_only]\n",
    "            text_only_ids = self._extract_text_only_sequence(input_ids)\n",
    "            \n",
    "            # Create combined embeddings: [B, T_text_only] -> [B, T_combined, D_lm]\n",
    "            combined_embeddings = self._insert_image_features_into_sequence(text_only_ids, image_features)\n",
    "            \n",
    "            # Generate using LLM tower: [B, T_combined, D_lm] -> [B, max_new_tokens]\n",
    "            return self._generate_with_llm_tower(combined_embeddings, max_new_tokens, \n",
    "                                               top_k, top_p, temperature, greedy)\n",
    "        \n",
    "        else:  # use_tower == 'both'\n",
    "            # Generate from both towers and return both results\n",
    "            vlm_output = self.vlm_tower.generate(input_ids, images, attention_mask, \n",
    "                                               max_new_tokens, top_k, top_p, temperature, greedy)\n",
    "            \n",
    "            llm_output = self.generate(input_ids, images, attention_mask, max_new_tokens, \n",
    "                                     top_k, top_p, temperature, greedy, use_tower='llm')\n",
    "            \n",
    "            return {'vlm_output': vlm_output, 'llm_output': llm_output}\n",
    "\n",
    "    def _generate_with_llm_tower(self, combined_embeddings, max_new_tokens=5, \n",
    "                               top_k=50, top_p=0.9, temperature=0.5, greedy=False):\n",
    "        \"\"\"\n",
    "        Generate text using the LLM tower with combined embeddings.\n",
    "        \n",
    "        Shape flow:\n",
    "        - combined_embeddings: [B, T_combined, D_lm] input sequence\n",
    "        - prefill_output: [B, T_combined, D_lm] after first forward pass\n",
    "        - current_logits: [B, vocab_size] logits for next token prediction\n",
    "        - next_token_id: [B, 1] sampled next token\n",
    "        - generated_ids: [B, max_new_tokens] final generated sequence\n",
    "        \"\"\"\n",
    "        current_total_seq_len = combined_embeddings.size(1)  # T_combined\n",
    "        batch_size = combined_embeddings.size(0)  # B\n",
    "        \n",
    "        # Prefill phase: [B, T_combined, D_lm] -> [B, T_combined, D_lm]\n",
    "        prefill_output, kv_cache_list = self.llm_tower(\n",
    "            combined_embeddings,\n",
    "            attention_mask=None,\n",
    "            kv_cache=None,\n",
    "            start_pos=0\n",
    "        )\n",
    "        \n",
    "        # Get last token output: [B, T_combined, D_lm] -> [B, D_lm]\n",
    "        last_token_output = prefill_output[:, -1, :]\n",
    "        # Convert to logits: [B, D_lm] -> [B, vocab_size]\n",
    "        current_logits = self.llm_tower.head(last_token_output)\n",
    "        \n",
    "        newly_generated_ids_list = []\n",
    "        \n",
    "        # Decode phase: generate max_new_tokens tokens autoregressively\n",
    "        for _ in range(max_new_tokens):\n",
    "            if greedy:\n",
    "                # Greedy sampling: [B, vocab_size] -> [B, 1]\n",
    "                next_token_id = torch.argmax(current_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                # Top-k/top-p sampling: [B, vocab_size] -> [B, vocab_size] -> [B, 1]\n",
    "                filtered_logits = top_k_top_p_filtering(current_logits, top_k=top_k, top_p=top_p)\n",
    "                probs = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            newly_generated_ids_list.append(next_token_id)  # Store [B, 1]\n",
    "            \n",
    "            # Embed the newly generated token: [B, 1] -> [B, 1, D_lm]\n",
    "            next_token_embed = self.llm_tower.token_embedding(next_token_id)\n",
    "            \n",
    "            current_token_start_pos = current_total_seq_len\n",
    "            current_total_seq_len += 1\n",
    "            \n",
    "            # Process with KV cache: [B, 1, D_lm] -> [B, 1, D_lm]\n",
    "            decode_step_output, kv_cache_list = self.llm_tower(\n",
    "                next_token_embed,\n",
    "                attention_mask=None,\n",
    "                kv_cache=kv_cache_list,\n",
    "                start_pos=current_token_start_pos\n",
    "            )\n",
    "            \n",
    "            # Get output for next prediction: [B, 1, D_lm] -> [B, D_lm] -> [B, vocab_size]\n",
    "            last_token_output = decode_step_output[:, -1, :]\n",
    "            current_logits = self.llm_tower.head(last_token_output)\n",
    "        \n",
    "        if not newly_generated_ids_list:\n",
    "            return torch.empty((batch_size, 0), dtype=torch.long, device=combined_embeddings.device)\n",
    "        \n",
    "        # Concatenate all generated tokens: list of [B, 1] -> [B, max_new_tokens]\n",
    "        generated_ids = torch.cat(newly_generated_ids_list, dim=1)\n",
    "        \n",
    "        # Handle EOS token post-processing\n",
    "        if self.tokenizer.eos_token_id is not None and generated_ids.numel() > 0:\n",
    "            seq_len = generated_ids.size(1)  # max_new_tokens\n",
    "            device = generated_ids.device\n",
    "            \n",
    "            # Find EOS positions: [B, max_new_tokens] -> boolean mask\n",
    "            eos_mask = (generated_ids == self.tokenizer.eos_token_id)\n",
    "            # Column indices: [max_new_tokens]\n",
    "            col_indices = torch.arange(seq_len, device=device)\n",
    "            # Masked indices: [B, max_new_tokens] where EOS positions have their column index\n",
    "            masked_col_indices = torch.where(eos_mask, \n",
    "                                           col_indices.unsqueeze(0).expand_as(generated_ids), \n",
    "                                           seq_len + 1)\n",
    "            \n",
    "            # Find first EOS in each batch: [B, max_new_tokens] -> [B]\n",
    "            first_eos_indices = torch.min(masked_col_indices, dim=1).values\n",
    "            actual_first_eos_indices = torch.clamp(first_eos_indices, max=seq_len)\n",
    "            \n",
    "            # Create replacement mask: tokens after first EOS should be replaced\n",
    "            col_indices_comparison = torch.arange(seq_len, device=device).unsqueeze(0).expand_as(generated_ids)\n",
    "            replace_mask = col_indices_comparison > actual_first_eos_indices.unsqueeze(1)\n",
    "            \n",
    "            # Replace tokens after EOS with EOS token\n",
    "            generated_ids[replace_mask] = self.tokenizer.eos_token_id\n",
    "        \n",
    "        return generated_ids  # [B, max_new_tokens]\n",
    "\n",
    "    def save_pretrained(self, save_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save both towers and configurations to a directory.\n",
    "        \n",
    "        Directory structure:\n",
    "        save_directory/\n",
    "        ├── vlm_tower/\n",
    "        │   ├── config.json\n",
    "        │   └── model.safetensors\n",
    "        ├── llm_tower/\n",
    "        │   ├── config.json\n",
    "        │   └── model.safetensors\n",
    "        └── twin_tower_config.json\n",
    "        \"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save VLM tower\n",
    "        vlm_dir = os.path.join(save_directory, \"vlm_tower\")\n",
    "        self.vlm_tower.save_pretrained(vlm_dir)\n",
    "        \n",
    "        # Save LLM tower\n",
    "        llm_dir = os.path.join(save_directory, \"llm_tower\")\n",
    "        os.makedirs(llm_dir, exist_ok=True)\n",
    "        with open(os.path.join(llm_dir, \"config.json\"), \"w\") as f:\n",
    "            f.write(json.dumps(asdict(self.llm_cfg), indent=4))\n",
    "        save_model(self.llm_tower, os.path.join(llm_dir, \"model.safetensors\"))\n",
    "        \n",
    "        # Save twin tower config\n",
    "        twin_config = {\n",
    "            \"vlm_config\": asdict(self.vlm_cfg),\n",
    "            \"llm_config\": asdict(self.llm_cfg),\n",
    "            \"architecture\": \"twin_tower\"\n",
    "        }\n",
    "        with open(os.path.join(save_directory, \"twin_tower_config.json\"), \"w\") as f:\n",
    "            f.write(json.dumps(twin_config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b97e0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_cfg = VLMConfig()\n",
    "llm_cfg = VLMConfig()\n",
    "twin_model = TwinTowerVLM(vlm_cfg, llm_cfg, load_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0073a715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwinTowerVLM(\n",
       "  (vlm_tower): VisionLanguageModel(\n",
       "    (vision_encoder): ViT(\n",
       "      (patch_embedding): ViTPatchEmbeddings(\n",
       "        (conv): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x ViTBlock(\n",
       "          (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): ViTMultiHeadAttention(\n",
       "            (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): ViTMLP(\n",
       "            (activation_fn): GELU(approximate='tanh')\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LanguageModel(\n",
       "      (token_embedding): Embedding(49218, 960)\n",
       "      (rotary_embd): RotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x LanguageModelBlock(\n",
       "          (mlp): LanguageModelMLP(\n",
       "            (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "            (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "            (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          )\n",
       "          (attn): LanguageModelGroupedQueryAttention(\n",
       "            (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "            (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "            (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "            (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm1): RMSNorm()\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "      (head): Linear(in_features=960, out_features=49218, bias=False)\n",
       "    )\n",
       "    (MP): ModalityProjector(\n",
       "      (proj): Linear(in_features=12288, out_features=960, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (llm_tower): LanguageModel(\n",
       "    (token_embedding): Embedding(49218, 960)\n",
       "    (rotary_embd): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x LanguageModelBlock(\n",
       "        (mlp): LanguageModelMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "        )\n",
       "        (attn): LanguageModelGroupedQueryAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm1): RMSNorm()\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (head): Linear(in_features=960, out_features=49218, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232ae18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
