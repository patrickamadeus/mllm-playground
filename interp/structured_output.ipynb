{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a8c7d-30d4-4ff4-9663-6eded5fcff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install transformers datasets outlines pillow accelerate qwen-vl-utils\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bf17d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd11a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f93940",
   "metadata": {},
   "source": [
    "# Outlines vs Usual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841a903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30fbcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "# MODEL_ID = \"yfan1997/GRIT-20-Qwen2.5-VL-3B\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "MODEL_CLASS = Qwen2_5_VLForConditionalGeneration\n",
    "PROCESSOR_CLASS = AutoProcessor\n",
    "\n",
    "MIN_PIXELS = 20*20\n",
    "MAX_PIXELS = 1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1629a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlines_model_processor(path, MODEL_CLASS, PROCESSOR_CLASS):\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"attn_implementation\": \"eager\",\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    processor_kwargs = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"use_fast\": True,\n",
    "    }\n",
    "\n",
    "    model = outlines.from_transformers(\n",
    "        MODEL_CLASS.from_pretrained(MODEL_ID, **model_kwargs),\n",
    "        PROCESSOR_CLASS.from_pretrained(MODEL_ID, **processor_kwargs),\n",
    "    )\n",
    "    return model, model.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91521f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_model_processor(path, MODEL_CLASS, PROCESSOR_CLASS):\n",
    "    model = MODEL_CLASS.from_pretrained(\n",
    "        path,\n",
    "        torch_dtype=DTYPE,\n",
    "        attn_implementation=\"eager\",\n",
    "        device_map=DEVICE,\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).eval()\n",
    "    processor = PROCESSOR_CLASS.from_pretrained(\n",
    "        path, \n",
    "        trust_remote_code=True, \n",
    "        padding_side='left', \n",
    "        use_fast=True, \n",
    "        min_pixels=MIN_PIXELS, max_pixels=MAX_PIXELS\n",
    "    )\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86041f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:10<00:00,  2.17s/it]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "## USE OUTLINES LIBRARY FOR STRUCTURED OUTPUT\n",
    "# model, processor = get_outlines_modelMODEL_ID, MODEL_CLASS, PROCESSOR_CLASS)\n",
    "\n",
    "## LOAD FOR REGULAR TRANSFORMER\n",
    "model, processor = get_hf_model_processor(MODEL_ID, MODEL_CLASS, PROCESSOR_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a5e0f-9ea6-4395-ba42-09fe5887d33d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Define the Structured Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ObjectCount(BaseModel):\n",
    "    count: int = Field(..., description=\"Number of objects in the image\")\n",
    "\n",
    "object_count_generator = outlines.Generator(model, ObjectCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2e1ae",
   "metadata": {},
   "source": [
    "# Inference (with `outlines`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc1e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How many red circles in the image?\"\n",
    "image_path = \"../files/img1.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "image_inputs, _ = process_vision_info(messages)\n",
    "input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[input_text],\n",
    "    images=image_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1476870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>How many red circles in the image?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993deb3-a60d-433e-b6a3-862cbe240f76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inference (with `transformers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba191768-53f8-490f-a396-ecef035467f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "MAX_NEW_TOKENS = 50\n",
    "TEMP = 0.0\n",
    "DO_SAMPLE = False\n",
    "OUT_ATTN = True\n",
    "RETURN_DICT = True\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=DO_SAMPLE,\n",
    "    output_attentions=OUT_ATTN,\n",
    "    return_dict_in_generate=RETURN_DICT,\n",
    "    temperature=TEMP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48af0eb4-b64e-4a47-8378-2f2d31b71e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = processor.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "# equivalent to batch decoding\n",
    "decoded_outputs = processor.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "decoded_output = decoded_outputs[0] # ~ first item in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bf5bb7-da61-4bd0-a916-16c319caf10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nYou are a helpful assistant.\\nuser\\nHow many red circles in the image?\\n addCriterion\\nThe image contains 12 red circles.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92bc2b-abc5-4d10-b43a-114c4c4a1bee",
   "metadata": {},
   "source": [
    "If wanna trim the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc219271-2d06-4b96-bdd4-0cd2c31b0f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' addCriterion\\nThe image contains 12 red circles.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = len(inputs['input_ids'][0])\n",
    "decoded_output = processor.tokenizer.decode(outputs.sequences[0][input_length:], skip_special_tokens=True)\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29ffdf-07c5-471b-88f4-43cb8d621070",
   "metadata": {},
   "source": [
    "## Use `outlines` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50549e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = object_count_generator({\n",
    "    \"text\": input_text,\n",
    "    \"images\": Image.open(image_path).convert(\"RGB\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ec15bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"count\": 11131 }\n",
      "11131\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "print(res)\n",
    "print(ast.literal_eval(res)['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59027c8c",
   "metadata": {},
   "source": [
    "# Prepare Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4831804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"patrickamadeus/multitask-diagnostic-suite-vlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "144a09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def input_ids_splitter(processor, text_query, image_inputs, video_inputs=None):\n",
    "    processor_kwargs = {\n",
    "        \"text\": [text_query],\n",
    "        \"images\": image_inputs,\n",
    "        \"padding\": True,\n",
    "        \"return_tensors\": \"pt\",\n",
    "    }\n",
    "    if video_inputs is not None:\n",
    "        processor_kwargs[\"videos\"] = video_inputs\n",
    "\n",
    "    inputs = processor(**processor_kwargs).to(DEVICE)\n",
    "    input_ids = inputs['input_ids'][0].tolist()\n",
    "\n",
    "    im_start_token_id = processor.tokenizer.convert_tokens_to_ids('<|im_start|>')\n",
    "    im_end_token_id = processor.tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
    "    vision_start_token_id = processor.tokenizer.convert_tokens_to_ids('<|vision_start|>')\n",
    "    vision_end_token_id = processor.tokenizer.convert_tokens_to_ids('<|vision_end|>')\n",
    "    image_pad_token_id = processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
    "\n",
    "    vision_start_pos = input_ids.index(vision_start_token_id)\n",
    "    vision_end_pos = input_ids.index(vision_end_token_id)\n",
    "    start_pos = vision_start_pos + 1\n",
    "    end_pos = vision_end_pos\n",
    "\n",
    "    im_start_positions = [i for i, tid in enumerate(input_ids) if tid == im_start_token_id]\n",
    "    im_end_positions = [i for i, tid in enumerate(input_ids) if tid == im_end_token_id]\n",
    "\n",
    "    system_indices = []\n",
    "    if im_start_positions and im_end_positions:\n",
    "        for start_idx in im_start_positions:\n",
    "            lookahead = input_ids[start_idx+1:start_idx+8]\n",
    "            lookahead_text = processor.tokenizer.decode(lookahead, skip_special_tokens=False)\n",
    "            if lookahead_text.startswith('system\\n'):\n",
    "                end_idx = next((e for e in im_end_positions if e > start_idx), None)\n",
    "                if end_idx is not None:\n",
    "                    system_indices = list(range(start_idx+1, end_idx))\n",
    "                break\n",
    "\n",
    "    text_indices = []\n",
    "    for start_idx in im_start_positions:\n",
    "        lookahead = input_ids[start_idx+1:start_idx+8]\n",
    "        lookahead_text = processor.tokenizer.decode(lookahead, skip_special_tokens=False)\n",
    "        if not lookahead_text.startswith('system\\n'):\n",
    "            end_idx = next((e for e in im_end_positions if e > start_idx), None)\n",
    "            if end_idx is not None:\n",
    "                for i in range(start_idx+1, end_idx):\n",
    "                    tid = input_ids[i]\n",
    "                    if tid not in [vision_start_token_id, image_pad_token_id]:\n",
    "                        text_indices.append(i)\n",
    "            break\n",
    "\n",
    "    special_token_indices = []\n",
    "    image_indices = []\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        token = processor.tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "        if token.startswith('<|') and token.endswith('|>') and token != '<|image_pad|>':\n",
    "            special_token_indices.append(i)\n",
    "        elif token_id == image_pad_token_id:\n",
    "            image_indices.append(i)\n",
    "\n",
    "    system_tokens = [input_ids[i] for i in system_indices]\n",
    "    vision_tokens = [input_ids[i] for i in image_indices]\n",
    "    text_tokens = [input_ids[i] for i in text_indices]\n",
    "    special_tokens = [input_ids[i] for i in special_token_indices]\n",
    "\n",
    "    system_text = processor.tokenizer.decode(system_tokens, skip_special_tokens=False)\n",
    "    vision_text = processor.tokenizer.decode(vision_tokens, skip_special_tokens=False)\n",
    "    text_text = processor.tokenizer.decode(text_tokens, skip_special_tokens=False)\n",
    "    special_text = processor.tokenizer.decode(special_tokens, skip_special_tokens=False)\n",
    "\n",
    "    locator_info = {\n",
    "        \"inputs\": inputs,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"system_indices\": system_indices,\n",
    "        \"image_indices\": image_indices,\n",
    "        \"text_indices\": text_indices,\n",
    "        \"special_token_indices\": special_token_indices,\n",
    "        \"system_tokens\": system_tokens,\n",
    "        \"vision_tokens\": vision_tokens,\n",
    "        \"text_tokens\": text_tokens,\n",
    "        \"special_tokens\": special_tokens,\n",
    "        \"system_text\": system_text,\n",
    "        \"vision_text\": vision_text,\n",
    "        \"text_text\": text_text,\n",
    "        \"special_text\": special_text,\n",
    "        \"vision_start_pos\": start_pos,\n",
    "        \"vision_end_pos\": end_pos,\n",
    "    }\n",
    "    \n",
    "    # Clean up temporary variables\n",
    "    del processor_kwargs, system_tokens, vision_tokens, text_tokens, special_tokens\n",
    "    del system_text, vision_text, text_text, special_text\n",
    "    del im_start_token_id, im_end_token_id, vision_start_token_id, vision_end_token_id, image_pad_token_id\n",
    "    del vision_start_pos, vision_end_pos, start_pos, end_pos\n",
    "    \n",
    "    return locator_info\n",
    "\n",
    "def extract_prompt_chunks(prompt: str):\n",
    "    im_start = \"<|im_start|>\"\n",
    "    im_end = \"<|im_end|>\"\n",
    "    vision_start = \"<|vision_start|>\"\n",
    "    vision_end = \"<|vision_end|>\"\n",
    "\n",
    "    # Extract system chunk\n",
    "    sys_start = prompt.find(f\"{im_start}system\\n\")\n",
    "    sys_end = prompt.find(im_end, sys_start)\n",
    "    system_chunk = None\n",
    "    if sys_start != -1 and sys_end != -1:\n",
    "        system_chunk = prompt[sys_start:sys_end+len(im_end)]\n",
    "\n",
    "    # Extract user chunk\n",
    "    user_start = prompt.find(f\"{im_start}user\\n\")\n",
    "    user_end = prompt.find(im_end, user_start)\n",
    "    user_chunk = None\n",
    "    if user_start != -1 and user_end != -1:\n",
    "        user_chunk = prompt[user_start:user_end+len(im_end)]\n",
    "\n",
    "    # Extract vision chunk (inside user chunk)\n",
    "    vision_chunk = None\n",
    "    if user_chunk is not None:\n",
    "        v_start = user_chunk.find(vision_start)\n",
    "        v_end = user_chunk.find(vision_end, v_start)\n",
    "        if v_start != -1 and v_end != -1:\n",
    "            vision_chunk = user_chunk[v_start:v_end+len(vision_end)]\n",
    "\n",
    "    # Extract user text (user chunk minus vision chunk)\n",
    "    user_text_chunk = None\n",
    "    if user_chunk is not None and vision_chunk is not None:\n",
    "        before = user_chunk[:user_chunk.find(vision_start)]\n",
    "        after = user_chunk[user_chunk.find(vision_end)+len(vision_end):]\n",
    "        user_text_chunk = (before + after).strip()\n",
    "        if user_text_chunk.startswith(f\"{im_start}user\\n\"):\n",
    "            user_text_chunk = user_text_chunk[len(f\"{im_start}user\\n\"):]\n",
    "        if user_text_chunk.endswith(im_end):\n",
    "            user_text_chunk = user_text_chunk[:-len(im_end)]\n",
    "        user_text_chunk = user_text_chunk.strip()\n",
    "\n",
    "    return {\n",
    "        \"system\": system_chunk,\n",
    "        \"user\": user_text_chunk,\n",
    "        \"vision\": vision_chunk\n",
    "    }\n",
    "\n",
    "def reorder_prompt_chunks(prompt: str, order: list):\n",
    "    chunks = extract_prompt_chunks(prompt)\n",
    "    chunk_map = {\n",
    "        \"system\": chunks.get(\"system\", \"\"),\n",
    "        \"user\": chunks.get(\"user\", \"\"),\n",
    "        \"vision\": chunks.get(\"vision\", \"\")\n",
    "    }\n",
    "    if chunk_map[\"user\"]:\n",
    "        chunk_map[\"user\"] = f\"<|im_start|>user\\n{chunk_map['user']}<|im_end|>\"\n",
    "    reordered = \"\"\n",
    "    for key in order:\n",
    "        if chunk_map[key]:\n",
    "            reordered += chunk_map[key]\n",
    "            if not reordered.endswith(\"\\n\"):\n",
    "                reordered += \"\\n\"\n",
    "    return reordered\n",
    "\n",
    "\n",
    "def build_prompt(question, image, processor, ordering):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    prompt = reorder_prompt_chunks(prompt, ordering)\n",
    "\n",
    "    tokenized_prompt = processor(\n",
    "        text=[prompt],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return tokenized_prompt, prompt\n",
    "\n",
    "def extract_number(text):\n",
    "    return int(re.search(r'\\d+', text).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28b88188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds['train'].to_pandas()\n",
    "df = df.loc[(df.task == 'object counting')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "973a3151-2b54-40a1-aa37-dadf5df3975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = df.iloc[0][\"image\"]\n",
    "img = Image.open(BytesIO(img['bytes'])).convert(\"RGB\")\n",
    "q = df.iloc[0][\"question\"] + \" Count:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "038850d6-f6fd-476a-8f6a-000fbb1edd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many ants are in the image? Count:'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32f3c6ed-b7ac-4bd0-8dd8-fd95f77d0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,prompt = build_prompt(q, img, processor, ['system','vision','user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f46b4eb-38cc-46bb-9e7b-89aaa5f1c572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "How many ants are in the image? Count:<|im_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58407ae0-0e7c-43ae-91ae-c4cea5764e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    output_attentions=False,\n",
    "    return_dict_in_generate=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "input_length = len(inputs['input_ids'][0])\n",
    "res_text = processor.tokenizer.decode(outputs.sequences[0][input_length:], skip_special_tokens=True)\n",
    "res = safe_int_parse(res_text)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "143a0366-8ccd-4ef6-a588-d4d10305df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' addCriterion\\nThe image contains 10 ants.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569d502-74eb-4382-a115-6799cd4976c2",
   "metadata": {},
   "source": [
    "# Final Evaluation loop (toggle `USE_OUTLINES`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cb0f7-e054-4e55-ad33-35c4f992d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "USE_OUTLINES = False\n",
    "OUTPUT_PATH = \"../files/results/output_trf_7b_455.jsonl\"\n",
    "PROMPT_SET_ORDERING = [\n",
    "    ['system', 'user', 'vision'],\n",
    "    ['system', 'vision', 'user'],\n",
    "    ['user', 'system', 'vision'],\n",
    "    ['user', 'vision', 'system'],\n",
    "    ['vision', 'system', 'user'],\n",
    "    ['vision', 'user', 'system'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bcb499e-828d-4847-b9d2-9dff17c50e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json, ast\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def safe_int_parse(x):\n",
    "    for fn in (\n",
    "        lambda v: int(ast.literal_eval(v)['count']),\n",
    "        lambda v: int(str(v).strip()),\n",
    "        lambda v: extract_number(str(v))\n",
    "    ):\n",
    "        try:\n",
    "            return fn(x)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def mini_bar(curr, total, width=20):\n",
    "    p = curr / total\n",
    "    nfull = int(p * width)\n",
    "    rem = width - nfull - 1\n",
    "    frac_idx = int((p * width % 1) * 8)\n",
    "    bars = \"▏▎▍▌▋▊▉█\"\n",
    "    frac = bars[frac_idx] if 0 <= frac_idx < len(bars) else \"\"\n",
    "    return f\"{'█'*nfull}{frac}{' '*(rem if rem>0 else 0)} {curr}/{total}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4a954f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing orders: 100%|██████████| 6/6 [27:23<00:00, 273.91s/it, ['vision', 'user', 'system'] | ████████████████████▏ 308/308]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json, ast\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def safe_int_parse(x):\n",
    "    for fn in (\n",
    "        lambda v: int(ast.literal_eval(v)['count']),\n",
    "        lambda v: int(str(v).strip()),\n",
    "        lambda v: extract_number(str(v))\n",
    "    ):\n",
    "        try:\n",
    "            return fn(x)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def mini_bar(curr, total, width=20):\n",
    "    p = curr / total\n",
    "    nfull = int(p * width)\n",
    "    rem = width - nfull - 1\n",
    "    frac_idx = int((p * width % 1) * 8)\n",
    "    bars = \"▏▎▍▌▋▊▉█\"\n",
    "    frac = bars[frac_idx] if 0 <= frac_idx < len(bars) else \"\"\n",
    "    return f\"{'█'*nfull}{frac}{' '*(rem if rem>0 else 0)} {curr}/{total}\"\n",
    "\n",
    "# Open file once and write row by row immediately\n",
    "with open(OUTPUT_PATH, \"a\") as f:\n",
    "    with tqdm(total=len(PROMPT_SET_ORDERING), desc=\"Processing orders\") as outer_pbar:\n",
    "        for order in PROMPT_SET_ORDERING:\n",
    "            for i, row in enumerate(df.itertuples(), 1):\n",
    "                answer = extract_number(row.answer)\n",
    "                img = Image.open(BytesIO(row.image['bytes'])).convert(\"RGB\")\n",
    "                inputs, _ = build_prompt(row.question+\" Count (in number):\", img, processor, order)\n",
    "\n",
    "                if USE_OUTLINES:\n",
    "                    res_text = object_count_generator({\n",
    "                        \"text\": input_text,\n",
    "                        \"images\": img\n",
    "                    })\n",
    "                    res = safe_int_parse(res_text)\n",
    "                else:\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=50,\n",
    "                        do_sample=False,\n",
    "                        output_attentions=False,\n",
    "                        return_dict_in_generate=True,\n",
    "                        temperature=0.0,\n",
    "                    )\n",
    "                    input_length = len(inputs['input_ids'][0])\n",
    "                    res_text = processor.tokenizer.decode(outputs.sequences[0][input_length:], skip_special_tokens=True)\n",
    "                    res = safe_int_parse(res_text)\n",
    "\n",
    "                # Immediately write each row\n",
    "                f.write(json.dumps({\n",
    "                    \"id\": row.Index,\n",
    "                    \"question\": row.question,\n",
    "                    \"answer\": answer,\n",
    "                    \"raw\": res_text,\n",
    "                    \"res\": res,\n",
    "                    \"abs_diff\": abs(res - answer) if res is not None else -1,\n",
    "                    \"order\": order,\n",
    "                }) + \"\\n\")\n",
    "                f.flush()  # make sure it's written to disk\n",
    "\n",
    "                # Update mini inner progress bar in outer_pbar\n",
    "                outer_pbar.set_postfix_str(f\"{order} | {mini_bar(i, len(df))}\")\n",
    "\n",
    "            outer_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4efab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c03291c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.52.3\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (0.34.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.52.3)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.3) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.3) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.3) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.3) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.3) (2025.1.31)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m402.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.0\n",
      "    Uninstalling tokenizers-0.22.0:\n",
      "      Successfully uninstalled tokenizers-0.22.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.56.1\n",
      "    Uninstalling transformers-4.56.1:\n",
      "      Successfully uninstalled transformers-4.56.1\n",
      "Successfully installed tokenizers-0.21.4 transformers-4.52.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.52.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c031b2-e929-4479-90d1-1a3c2731e6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
